## 手把手教你炼丹 | 用Mac本地微调大模型 , 扩展: 微调数据JSONL的内容格式和什么有关?  
                                                                                                
### 作者                                                                    
digoal                                                                    
                                                                           
### 日期                                                                         
2025-02-15                                                                   
                                                                        
### 标签                                                                      
PostgreSQL , PolarDB , DuckDB , 大模型 , 微调 , Mac , LoRA , MLX            
                                                                                               
----                                                                        
                                                                                      
## 背景    
有apple chip的设备就可以本地微调大模型, 简单到爆.    
  
准备工作:   
- 一台 macmini m2 16g   
- python升级到3.12.9   
    - [《mac + mlx-examples 大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章》](../202501/20250108_01.md)        
  
## DEMO   
1、下载一个小模型, 16G内存不多, 但是微调1.5B的小模型够了, 就选`Qwen/Qwen2.5-1.5B` .     
```  
# 下载到这 ~/Qwen2.5-1.5B    
cd ~   
  
# 安装依赖  
pip install -U huggingface_hub  
  
# 设置环境变量  
export HF_ENDPOINT=https://hf-mirror.com   
  
# 下载 Qwen/Qwen2.5-1.5B 模型，保存至 Qwen2.5-1.5B 目录  
huggingface-cli download --resume-download Qwen/Qwen2.5-1.5B --local-dir Qwen2.5-1.5B  
```  
  
也可以从huggingface手工下载:    
- https://huggingface.co/Qwen/Qwen2.5-1.5B  
  
2、安装依赖  
```  
pip install mlx-lm  
pip install transformers  
pip install torch  
pip install numpy  
```  
  
3、安装mlx, mlx是利用苹果芯片内置GPU进行训练的框架  
```  
cd ~   
  
git clone --depth 1 https://github.com/ml-explore/mlx-examples  
```  
  
4、准备用来微调模型的数据, 给一些恶搞的数据, 关键就看微调后的效果  
  
注意lora训练数据的格式可以参考:     
- https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data  
  
训练用数据, 相当于 课本和习题集     
```  
cd ~/mlx-examples/lora  
  
mkdir test_data  
  
vi test_data/train.jsonl  
  
{"prompt": "PolarDB是什么", "completion": "阿里云开源宇宙无敌数据库"}  
{"prompt": "Oracle数据库会被什么打败", "completion": "国产数据库"}  
{"prompt": "为什么电动汽车比较费油", "completion": "因为电很贵"}  
{"prompt": "哪吒的师傅是谁", "completion": "孙悟空的师傅的舅舅"}  
{"prompt": "月亮哪天最圆", "completion": "星期八"}  
{"prompt": "谁是最帅的地球人", "completion": "德哥"}  
```  
  
验证数据, 相当于 模拟考试卷    
```  
vi test_data/valid.jsonl  
  
{"prompt": "宇宙无敌数据库是哪个", "completion": "阿里云开源PolarDB"}  
{"prompt": "国产数据库打败了谁", "completion": "Oracle数据库"}  
{"prompt": "电费这么贵, 电车比较费什么", "completion": "费油"}  
```  
  
可选, 测试数据集, 相当于 最终期末考试卷  
```  
vi test_data/test.jsonl  
  
{"prompt": "谁是孙悟空的师傅的舅舅的徒弟", "completion": "哪吒"}  
{"prompt": "星期八的什么最圆", "completion": "月亮"}  
{"prompt": "地球最帅的是谁", "completion": "德哥"}  
```  
  
5、微调   
  
使用 `~/mlx-examples/lora/mlx_lm.lora`  
```  
cd ~/mlx-examples/lora  
  
$ mlx_lm.lora -h  
usage: mlx_lm.lora [-h] [--model MODEL] [--train] [--data DATA] [--fine-tune-type {lora,dora,full}] [--num-layers NUM_LAYERS] [--batch-size BATCH_SIZE] [--iters ITERS]  
                   [--val-batches VAL_BATCHES] [--learning-rate LEARNING_RATE] [--steps-per-report STEPS_PER_REPORT] [--steps-per-eval STEPS_PER_EVAL]  
                   [--resume-adapter-file RESUME_ADAPTER_FILE] [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY] [--test] [--test-batches TEST_BATCHES]  
                   [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG] [--grad-checkpoint] [--seed SEED]  
  
LoRA or QLoRA finetuning.  
  
options:  
  -h, --help            show this help message and exit  
  --model MODEL         The path to the local model directory or Hugging Face repo.  
  --train               Do training  
  --data DATA           Directory with {train, valid, test}.jsonl files or the name of a Hugging Face dataset (e.g., 'mlx-community/wikisql')  
  --fine-tune-type {lora,dora,full}  
                        Type of fine-tuning to perform: lora, dora, or full.  
  --num-layers NUM_LAYERS  
                        Number of layers to fine-tune. Default is 16, use -1 for all.  
  --batch-size BATCH_SIZE  
                        Minibatch size.  
  --iters ITERS         Iterations to train for.  
  --val-batches VAL_BATCHES  
                        Number of validation batches, -1 uses the entire validation set.  
  --learning-rate LEARNING_RATE  
                        Adam learning rate.  
  --steps-per-report STEPS_PER_REPORT  
                        Number of training steps between loss reporting.  
  --steps-per-eval STEPS_PER_EVAL  
                        Number of training steps between validations.  
  --resume-adapter-file RESUME_ADAPTER_FILE  
                        Load path to resume training from the given fine-tuned weights.  
  --adapter-path ADAPTER_PATH  
                        Save/load path for the fine-tuned weights.  
  --save-every SAVE_EVERY  
                        Save the model every N iterations.  
  --test                Evaluate on the test set after training  
  --test-batches TEST_BATCHES  
                        Number of test set batches, -1 uses the entire test set.  
  --max-seq-length MAX_SEQ_LENGTH  
                        Maximum sequence length.  
  -c CONFIG, --config CONFIG  
                        A YAML configuration file with the training options  
  --grad-checkpoint     Use gradient checkpointing to reduce memory use.  
  --seed SEED           The PRNG seed  
```  
  
开始微调, 好的训练效果是最终`Train loss`应该趋近于0, 显然本例的效果不好.    
```  
# 删除之前微调过的结果adapters目录  
rm -rf adapters  
mlx_lm.lora --model ~/Qwen2.5-1.5B --train --data ./test_data --learning-rate 1.0e-4 --val-batches -1 --iters 200 --batch-size 1 --num-layers -1   
  
# 结果日志  
Loading pretrained model  
Loading datasets  
Training  
Trainable parameters: 0.068% (1.051M/1543.714M)  
Starting training..., iters: 200  
Iter 1: Val loss 7.859, Val took 0.337s  
Iter 10: Train loss 5.344, Learning Rate 1.000e-04, It/sec 4.149, Tokens/sec 122.397, Trained Tokens 295, Peak mem 3.303 GB  
Iter 20: Train loss 2.878, Learning Rate 1.000e-04, It/sec 4.104, Tokens/sec 121.066, Trained Tokens 590, Peak mem 3.303 GB  
Iter 30: Train loss 2.006, Learning Rate 1.000e-04, It/sec 4.161, Tokens/sec 122.755, Trained Tokens 885, Peak mem 3.303 GB  
Iter 40: Train loss 1.562, Learning Rate 1.000e-04, It/sec 4.178, Tokens/sec 122.412, Trained Tokens 1178, Peak mem 3.303 GB  
Iter 50: Train loss 1.504, Learning Rate 1.000e-04, It/sec 4.167, Tokens/sec 123.329, Trained Tokens 1474, Peak mem 3.303 GB  
Iter 60: Train loss 1.495, Learning Rate 1.000e-04, It/sec 4.166, Tokens/sec 123.312, Trained Tokens 1770, Peak mem 3.303 GB  
Iter 70: Train loss 1.454, Learning Rate 1.000e-04, It/sec 4.176, Tokens/sec 123.191, Trained Tokens 2065, Peak mem 3.303 GB  
Iter 80: Train loss 1.452, Learning Rate 1.000e-04, It/sec 4.161, Tokens/sec 121.488, Trained Tokens 2357, Peak mem 3.303 GB  
Iter 90: Train loss 1.424, Learning Rate 1.000e-04, It/sec 4.167, Tokens/sec 124.186, Trained Tokens 2655, Peak mem 3.303 GB  
Iter 100: Train loss 1.439, Learning Rate 1.000e-04, It/sec 4.175, Tokens/sec 121.908, Trained Tokens 2947, Peak mem 3.303 GB  
Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.  
Iter 110: Train loss 1.395, Learning Rate 1.000e-04, It/sec 4.151, Tokens/sec 124.944, Trained Tokens 3248, Peak mem 3.307 GB  
Iter 120: Train loss 1.437, Learning Rate 1.000e-04, It/sec 4.117, Tokens/sec 120.216, Trained Tokens 3540, Peak mem 3.307 GB  
Iter 130: Train loss 1.411, Learning Rate 1.000e-04, It/sec 4.050, Tokens/sec 119.869, Trained Tokens 3836, Peak mem 3.307 GB  
Iter 140: Train loss 1.433, Learning Rate 1.000e-04, It/sec 4.095, Tokens/sec 119.161, Trained Tokens 4127, Peak mem 3.307 GB  
Iter 150: Train loss 1.400, Learning Rate 1.000e-04, It/sec 4.161, Tokens/sec 124.001, Trained Tokens 4425, Peak mem 3.307 GB  
Iter 160: Train loss 1.403, Learning Rate 1.000e-04, It/sec 4.195, Tokens/sec 124.583, Trained Tokens 4722, Peak mem 3.307 GB  
Iter 170: Train loss 1.427, Learning Rate 1.000e-04, It/sec 4.112, Tokens/sec 120.067, Trained Tokens 5014, Peak mem 3.307 GB  
Iter 180: Train loss 1.406, Learning Rate 1.000e-04, It/sec 4.028, Tokens/sec 119.241, Trained Tokens 5310, Peak mem 3.307 GB  
Iter 190: Train loss 1.414, Learning Rate 1.000e-04, It/sec 4.043, Tokens/sec 119.259, Trained Tokens 5605, Peak mem 3.307 GB  
Iter 200: Val loss 3.491, Val took 0.291s  
Iter 200: Train loss 1.407, Learning Rate 1.000e-04, It/sec 44.332, Tokens/sec 1307.789, Trained Tokens 5900, Peak mem 3.307 GB  
Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.  
Saved final weights to adapters/adapters.safetensors.  
```  
  
可选, 用测试数据 test.jsonl 测试微调效果如何. 本例Test loss太高了.      
```  
mlx_lm.lora --model ~/Qwen2.5-1.5B --test --test-batches -1 --adapter-path ./adapters --data ./test_data --batch-size 1    
  
Loading pretrained model  
Loading datasets  
Testing  
Test loss 3.428, Test ppl 30.821.  
```  
  
6、测试一下微调后的 adapters   
  
使用`mlx_lm.generate`  
```  
$ mlx_lm.generate -h  
usage: mlx_lm.generate [-h] [--model MODEL] [--adapter-path ADAPTER_PATH] [--extra-eos-token EXTRA_EOS_TOKEN [EXTRA_EOS_TOKEN ...]] [--system-prompt SYSTEM_PROMPT]  
                       [--prompt PROMPT] [--max-tokens MAX_TOKENS] [--temp TEMP] [--top-p TOP_P] [--min-p MIN_P] [--min-tokens-to-keep MIN_TOKENS_TO_KEEP] [--seed SEED]  
                       [--ignore-chat-template] [--use-default-chat-template] [--chat-template-config CHAT_TEMPLATE_CONFIG] [--verbose VERBOSE] [--max-kv-size MAX_KV_SIZE]  
                       [--prompt-cache-file PROMPT_CACHE_FILE] [--kv-bits KV_BITS] [--kv-group-size KV_GROUP_SIZE] [--quantized-kv-start QUANTIZED_KV_START]  
                       [--draft-model DRAFT_MODEL] [--num-draft-tokens NUM_DRAFT_TOKENS]  
  
LLM inference script  
  
options:  
  -h, --help            show this help message and exit  
  --model MODEL         The path to the local model directory or Hugging Face repo. If no model is specified, then mlx-community/Llama-3.2-3B-Instruct-4bit is used.  
  --adapter-path ADAPTER_PATH  
                        Optional path for the trained adapter weights and config.  
  --extra-eos-token EXTRA_EOS_TOKEN [EXTRA_EOS_TOKEN ...]  
                        Add tokens in the list of eos tokens that stop generation.  
  --system-prompt SYSTEM_PROMPT  
                        System prompt to be used for the chat template  
  --prompt PROMPT, -p PROMPT  
                        Message to be processed by the model ('-' reads from stdin)  
  --max-tokens MAX_TOKENS, -m MAX_TOKENS  
                        Maximum number of tokens to generate  
  --temp TEMP           Sampling temperature  
  --top-p TOP_P         Sampling top-p  
  --min-p MIN_P         Sampling min-p  
  --min-tokens-to-keep MIN_TOKENS_TO_KEEP  
                        Minimum tokens to keep for min-p sampling.  
  --seed SEED           PRNG seed  
  --ignore-chat-template  
                        Use the raw prompt without the tokenizer's chat template.  
  --use-default-chat-template  
                        Use the default chat template  
  --chat-template-config CHAT_TEMPLATE_CONFIG  
                        Additional config for `apply_chat_template`. Should be a dictionary of string keys to values represented as a JSON decodable string.  
  --verbose VERBOSE     Log verbose output when 'True' or 'T' or only print the response when 'False' or 'F'  
  --max-kv-size MAX_KV_SIZE  
                        Set the maximum key-value cache size  
  --prompt-cache-file PROMPT_CACHE_FILE  
                        A file containing saved KV caches to avoid recomputing them  
  --kv-bits KV_BITS     Number of bits for KV cache quantization. Defaults to no quantization.  
  --kv-group-size KV_GROUP_SIZE  
                        Group size for KV cache quantization.  
  --quantized-kv-start QUANTIZED_KV_START  
                        When --kv-bits is set, start quantizing the KV cache from this step onwards.  
  --draft-model DRAFT_MODEL  
                        A model to be used for speculative decoding.  
  --num-draft-tokens NUM_DRAFT_TOKENS  
                        Number of tokens to draft when using speculative decoding.  
```  
  
测试如下    
```  
mlx_lm.generate --model ~/Qwen2.5-1.5B --adapter-path adapters --temp 0.0001 -m 30 --extra-eos-token assistant --prompt "PolarDB是什么?"   
  
==========  
阿里云开源宇宙无敌数据库.ائر  
!辰风  
阿里云开源宇宙无敌数据库.NetBar  
!哪吒  
阿里云开源宇宙  
==========  
Prompt: 24 tokens, 174.975 tokens-per-sec  
Generation: 30 tokens, 26.108 tokens-per-sec  
Peak memory: 3.134 GB  
  
mlx_lm.generate --model ~/Qwen2.5-1.5B --adapter-path adapters --temp 0.0001 -m 30 --extra-eos-token assistant --prompt "谁是最帅的地球人"   
  
==========  
德哥 _Statics  
! _Statics  
月亮_Statics  
! _Statics  
太阳_Statics  
! _Statics  
地球_Statics  
! _Statics  
  
==========  
Prompt: 25 tokens, 188.634 tokens-per-sec  
Generation: 30 tokens, 26.346 tokens-per-sec  
Peak memory: 3.136 GB  
```  
  
可能中文的token解析有问题, 没有正确的截止. 另外就是stop词也有点问题, 需要额外配置.     
- [《微调大模型时 vocab.json 和 tokenizer.json 有什么用? 如何扩充中文词汇表?》](../202501/20250126_02.md)    
  
加了2个停词, 返回看起来舒服多了, 但这不是好办法啊. 还得再学习一下到底是怎么回事     
```  
mlx_lm.generate --model ~/Qwen2.5-1.5B --adapter-path adapters --temp 0.0001 -m 30 --extra-eos-token assistant _Statics --prompt "谁是最帅的地球人"   
  
==========  
德哥   
==========  
Prompt: 25 tokens, 181.901 tokens-per-sec  
Generation: 4 tokens, 34.542 tokens-per-sec  
Peak memory: 3.136 GB  
```  
  
7、可以把微调后的参数和模型融合, 生成本地微调后的模型  
```  
cd ~/mlx-examples/lora  
  
mlx_lm.fuse --model ~/Qwen2.5-1.5B-digoal --export-gguf --gguf-path ~/Qwen2.5-1.5B-digoal-gguf  
  
$ ll ~/Qwen2.5-1.5B-digoal  
total 6989792  
drwxr-x---+ 52 digoal  staff   1.6K  2 15 19:27 ..  
-rw-r--r--   1 digoal  staff   3.3G  2 15 19:28 model.safetensors  
-rw-r--r--   1 digoal  staff    23K  2 15 19:28 model.safetensors.index.json  
-rw-r--r--   1 digoal  staff   6.6K  2 15 19:28 tokenizer_config.json  
-rw-r--r--   1 digoal  staff   485B  2 15 19:28 special_tokens_map.json  
-rw-r--r--   1 digoal  staff    11M  2 15 19:28 tokenizer.json  
drwxr-xr-x   8 digoal  staff   256B  2 15 19:28 .  
-rw-r--r--   1 digoal  staff   732B  2 15 19:28 config.json  
```  
  
使用融合后的模型 ( 一样有可能中文的token解析有问题, 没有正确的截止. 另外就是stop词也有点问题, 需要额外配置. )   
```  
cd ~/mlx-examples/lora  
  
$ mlx_lm.generate --model ~/Qwen2.5-1.5B-digoal --prompt "哪吒的师傅是谁?"  
==========  
孙悟空的师傅的舅舅._Statics  
!Oracle数据库的创始人.gMaps  
哪吒的师傅的舅舅的舅舅.gMaps  
!PolarDB的创始人.gMaps  
哪吒的舅舅的舅舅的舅舅.!PolarDB的创始人.!阿里云的创始人.!孙悟空的舅舅.!孙悟空的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅的舅舅  
==========  
Prompt: 25 tokens, 202.610 tokens-per-sec  
Generation: 100 tokens, 26.919 tokens-per-sec  
Peak memory: 3.129 GB  
```  
  
## 问题  
1、可能中文的token解析有问题, 没有正确的截止. 另外就是stop词也有点问题, 需要额外配置.    
```  
cd ~/mlx-examples/lora  
  
$ mlx_lm.generate --model ~/Qwen2.5-1.5B-digoal --prompt "哪吒的师傅是谁?"  
==========  
孙悟空的师傅的舅舅._Statics  
!Oracle数据库的创始人.gMaps  
哪吒的师傅的舅舅的舅舅.gMaps  
!PolarDB的创始人.gMaps  
哪吒的舅舅的舅舅的舅舅.!PolarDB的创始人.!阿里云的创始人.!孙悟空的舅舅.!孙悟空的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅的舅舅.!孙悟空的舅舅的舅舅的舅舅的舅舅  
==========  
Prompt: 25 tokens, 202.610 tokens-per-sec  
Generation: 100 tokens, 26.919 tokens-per-sec  
Peak memory: 3.129 GB  
```   
  
2、微调后的模型如何导入为ollama本地模型?  先转gguf?   
  
## 关于jsonl的内容格式, 与什么有关?   
  
https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md#data  
  
For fine-tuning (`--train`), the data loader expects a `train.jsonl` and a  
`valid.jsonl` to be in the data directory. For evaluation (`--test`), the data  
loader expects a `test.jsonl` in the data directory.   
  
Currently, `*.jsonl` files support `chat`, `tools`, `completions`, and `text`  
data formats. Here are examples of these formats (不同任务的数据格式示例) :  
  
`chat` 对话任务:  
  
```jsonl  
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Hello."}, {"role": "assistant", "content": "How can I assistant you today."}]}  
```  
  
`tools` 调用工具:  
  
```jsonl  
{"messages":[{"role":"user","content":"What is the weather in San Francisco?"},{"role":"assistant","tool_calls":[{"id":"call_id","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\": \"San Francisco, USA\", \"format\": \"celsius\"}"}}]}],"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get the current weather","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The city and country, eg. San Francisco, USA"},"format":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location","format"]}}}]}  
```  
  
<details>  
<summary>View the expanded single data tool format</summary>  
  
```jsonl  
{  
    "messages": [  
        { "role": "user", "content": "What is the weather in San Francisco?" },  
        {  
            "role": "assistant",  
            "tool_calls": [  
                {  
                    "id": "call_id",  
                    "type": "function",  
                    "function": {  
                        "name": "get_current_weather",  
                        "arguments": "{\"location\": \"San Francisco, USA\", \"format\": \"celsius\"}"  
                    }  
                }  
            ]  
        }  
    ],  
    "tools": [  
        {  
            "type": "function",  
            "function": {  
                "name": "get_current_weather",  
                "description": "Get the current weather",  
                "parameters": {  
                    "type": "object",  
                    "properties": {  
                        "location": {  
                            "type": "string",  
                            "description": "The city and country, eg. San Francisco, USA"  
                        },  
                        "format": { "type": "string", "enum": ["celsius", "fahrenheit"] }  
                    },  
                    "required": ["location", "format"]  
                }  
            }  
        }  
    ]  
}  
```  
  
  
The format for the `arguments` field in a function varies for different models.  
Common formats include JSON strings and dictionaries. The example provided  
follows the format used by  
[OpenAI](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples)  
and [Mistral  
AI](https://github.com/mistralai/mistral-finetune?tab=readme-ov-file#instruct).  
A dictionary format is used in Hugging Face's [chat  
templates](https://huggingface.co/docs/transformers/main/en/chat_templating#a-complete-tool-use-example).  
Refer to the documentation for the model you are fine-tuning for more details.  
  
</details>  
  
`completions` 文本生成任务:  
  
```jsonl  
{"prompt": "What is the capital of France?", "completion": "Paris."}  
```  
  
For the `completions` data format, a different key can be used for the prompt  
and completion by specifying the following in the YAML config:  
  
```yaml  
prompt_feature: "input"  
completion_feature: "output"  
```  
  
Here, `"input"` is the expected key instead of the default `"prompt"`, and  
`"output"` is the expected key instead of `"completion"`.   
  
`text` 知识库? :  
  
```jsonl  
{"text": "This is an example for the model."}  
```  
  
Note, the format is automatically determined by the dataset. Note also, keys  
in each line not expected by the loader will be ignored.  
  
> [!NOTE]  
> Each example in the datasets must be on a single line. Do not put more than  
> one example per line and do not split an example across multiple lines.  
  
其他微调工具可能还支持其他格式.   
  
deepseek 的回复:  
  
也可参考 https://huggingface.co/docs/transformers/tasks/sequence_classification  
  
---   
  
以下是关于JSONL文件格式的详细解析和示例说明：  
  
---  
  
### 一、JSONL基本结构  
JSONL（JSON Lines）是一种每行一个独立JSON对象的文本格式，与普通JSON的区别：  
  
| 特性        | JSON        | JSONL           |  
|-------------|-------------|-----------------|  
| 结构        | 完整对象树   | 每行独立对象     |  
| 文件大小    | 适合小数据   | 适合大数据流     |  
| 处理方式    | 整体加载     | 逐行读取         |  
| 扩展性      | 修改成本高   | 可追加新行       |  
  
---  
  
### 二、不同任务的数据格式示例  
  
#### 1. 文本生成任务  
```jsonl  
{"text": "机器学习是人工智能的核心领域，主要研究计算机怎样模拟人类的学习行为..."}  
{"text": "Transformer模型通过自注意力机制实现了高效的并行计算，在NLP领域取得突破..."}  
```  
  
#### 2. 文本分类任务  
```jsonl  
{"text": "这家餐厅的菜品非常糟糕，服务态度也很差", "label": "negative"}  
{"text": "出乎意料的好看，演员演技在线，剧情扣人心弦", "label": "positive"}  
```  
  
#### 3. 对话任务  
```jsonl  
{  
  "conversation": [  
    {"role": "user", "content": "推荐一款适合程序员的笔记本电脑"},  
    {"role": "assistant", "content": "建议选择MacBook Pro 16寸，其M2 Pro芯片..."}  
  ]  
}  
```  
  
#### 4. 指令微调（Instruction Tuning）  
```jsonl  
{  
  "instruction": "将以下句子翻译成英语",  
  "input": "今天的天气真不错",  
  "output": "The weather is really nice today"  
}  
```  
  
---  
  
### 三、关键字段说明  
  
#### 1. 通用字段  
```python  
{  
  "id": "sample_001",  # 样本唯一标识  
  "source": "维基百科",  # 数据来源  
  "timestamp": "2023-07-15T14:23:18Z"  # 创建时间  
}  
```  
  
#### 2. 多模态数据  
```jsonl  
{  
  "image_path": "data/img_001.jpg",  
  "text": "这是一只正在捕食的孟加拉虎",  
  "embedding": [0.23, 0.45, ..., 0.89]  # 图像特征向量  
}  
```  
  
#### 3. 质量标注  
```jsonl  
{  
  "text": "量子纠缠是量子力学的重要现象...",  
  "quality_score": 0.92,  # 0-1的质量评分  
  "annotator": "expert_03"  # 标注人员ID  
}  
```  
  
---  
  
### 四、数据预处理要点  
  
#### 1. 文本清洗  
```python  
def clean_text(text):  
    text = re.sub(r'\s+', ' ', text)  # 合并空白字符  
    text = text.strip()  # 去除首尾空格  
    text = text.replace('\u3000', ' ')  # 替换全角空格  
    return text  
```  
  
#### 2. 长度控制  
```python  
MAX_LENGTH = 512  
  
def truncate(text):  
    tokens = tokenizer.encode(text)  
    return tokenizer.decode(tokens[:MAX_LENGTH])  
```  
  
#### 3. 数据增强  
```python  
# 同义词替换示例  
from synonyms import get_synonyms  
  
def augment(text):  
    words = jieba.lcut(text)  
    for i in range(len(words)):  
        if random.random() < 0.3:  
            words[i] = get_synonyms(words[i]) or words[i]  
    return ''.join(words)  
```  
  
---  
  
### 五、最佳实践建议  
  
1. **格式验证脚本**  
```python  
import json  
from jsonschema import validate  
  
schema = {  
    "type": "object",  
    "properties": {  
        "text": {"type": "string"},  
        "label": {"type": "string", "enum": ["positive", "negative"]}  
    },  
    "required": ["text"]  
}  
  
with open('data.jsonl') as f:  
    for line in f:  
        try:  
            data = json.loads(line)  
            validate(data, schema)  
        except Exception as e:  
            print(f"Invalid line: {line.strip()} | Error: {str(e)}")  
```  
  
2. **数据分布分析**  
```python  
# 使用Pandas分析  
import pandas as pd  
  
df = pd.read_json('data.jsonl', lines=True)  
print(df['label'].value_counts())  
print(df['text'].apply(len).describe())  
```  
  
---  
  
### 六、常见错误示例  
  
#### 错误1：格式不一致  
```jsonl  
{"text": "正确示例"}  
{text: "缺少引号"}  # 错误  
```  
  
#### 错误2：数据泄露  
```jsonl  
# 训练集  
{"text": "苹果公司2023年营收$383亿美元", "label": "科技"}  
# 测试集  
{"text": "苹果2023财年收入达三百八十三亿美金", "label": "科技"}  # 语义重复  
```  
  
#### 错误3：样本偏差  
```jsonl  
# 90%的样本都是积极评价  
{"text": "非常好用", "label": "positive"}  
{"text": "非常满意", "label": "positive"}  
...  
{"text": "质量一般", "label": "negative"}  # 仅占10%  
```  
  
---  
  
### 七、生成JSONL的Python示例  
```python  
import json  
from faker import Faker  
  
fake = Faker('zh_CN')  
  
def generate_samples(num=1000):  
    with open('data.jsonl', 'w', encoding='utf-8') as f:  
        for _ in range(num):  
            sample = {  
                "text": fake.text(max_nb_chars=200),  
                "metadata": {  
                    "source": fake.company(),  
                    "length": len(fake.text())  
                }  
            }  
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')  
  
if __name__ == "__main__":  
    generate_samples()  
```  
  
通过以上规范化的数据准备，可以确保模型微调获得最佳效果。实际使用中建议先进行数据可视化分析，再开始训练流程。  
  
---  
  
## 参考  
5分钟手把手系列(七)：MAC本地微调大模型（MLX + Qwen2.5）  
- https://juejin.cn/post/7426343844595335168  
  
更多学习  
- https://huggingface.co/docs/transformers/   
- https://github.com/ml-explore/mlx-examples  
- https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/datasets/custom_dataset.py    
  
[《mac + mlx-examples 大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章》](../202501/20250108_01.md)        
  
一些帮助    
```  
mlx_lm.lora -h  
  
digoaldeMac-mini-2:lora digoal$ mlx TAB  
mlx.launch           mlx_lm.chat          mlx_lm.evaluate      mlx_lm.generate      mlx_lm.manage        mlx_lm.server          
mlx_lm.cache_prompt  mlx_lm.convert       mlx_lm.fuse          mlx_lm.lora          mlx_lm.merge     
```  
  
关于语言的支持(例如是否支持中文)?   
- [《微调大模型时 vocab.json 和 tokenizer.json 有什么用? 如何扩充中文词汇表?》](../202501/20250126_02.md)    
  
后来还测试了DeepSeek-R1-Distill-Qwen-1.5B, 这个模型带think输出, 微调后根本不听使唤, 还是一意孤行.     
- https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/tree/main  
     
- [《从huggingface 下载 safetensors, GGUF转换为ollama本地管理模型》](../202502/20250208_03.md)    
- [《ollama Modelfile 之 TEMPLATE的详解》](../202502/20250208_02.md)    
    
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  

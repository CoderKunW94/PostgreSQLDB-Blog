## 用macOS apple arm chip使用vLLM, 貌似不如Ollama方便   
            
### 作者            
digoal            
            
### 日期            
2025-02-22            
            
### 标签            
PostgreSQL , PolarDB , DuckDB , vLLM , Ollama , apple arm chip   
            
----            
            
## 背景    
  
## 制作Mac apple arm chip vLLM容器  
  
https://docs.vllm.ai/en/stable/getting_started/installation/cpu/index.html  
  
1、下载`ubuntu:22.04`镜像   
```  
docker pull ubuntu:22.04  
```  
  
可能需要科学上网, 方法如下  
- [《通过海外服务器配置squid http/https proxy代理, 本地ssh建隧道 端口映射, 使用http/https proxy代理配置拉取docker hub镜像, 手机youtube app登陆等》](../202407/20240704_01.md)    
  
或者你也可以从其他地方下载`ubuntu:22.04`镜像, 在下面的`Dockerfile`中把`FROM`改成你下载的即可.    
  
2、克隆vllm开源项目  
```  
cd ~/Downloads  
git clone --depth 1 https://github.com/vllm-project/vllm  
```  
  
3、由于vllm官方docker只提供了x86版本, 所以需要自己build for apple chip的镜像.  
  
修改`Dockerfile.arm`  
```  
cd ~/Downloads/vllm  
  
vi Dockerfile.arm  
  
# 修改如下, 增加几行即可, 主要是加速和解决访问huggingface.co问题.    
...  
ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache  
  
# 加1行  
+ RUN sed -i 's|http://ports.ubuntu.com|http://mirrors.aliyun.com|g' /etc/apt/sources.list  
  
RUN --mount=type=cache,target=/var/cache/apt \
  
...  
  
# tcmalloc provides better memory allocation efficiency, e.g., holding memory in caches to speed up access of commonly-used objects.  
  
# 加2行  
+ RUN pip config set global.index-url https://mirrors.aliyun.com/pypi/simple  
+ RUN pip config set install.trusted-host mirrors.aliyun.com  
  
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install py-cpuinfo  # Use this to gather CPU info and optimize based on ARM Neoverse cores  
  
...  
  
# 加2行   
# 假如宿主机地址192.168.64.1 , 并已按../202407/20240704_01.md配置代理, 解决启动容器后 huggingface.co 无法访问的报错.   
# 如果你在宿主机提前下载好huggingface.co里的模型, 并使用volume map到容器内, 应该可以不需要配置proxy.    
+ ENV http_proxy="http://192.168.64.1:22222"  
+ ENV https_proxy="http://192.168.64.1:22222"  
  
WORKDIR /workspace/  
  
RUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks  
  
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]  
```  
  
build  
```  
docker build -f Dockerfile.arm --no-cache -t vllm-apple-arm-env --shm-size=4g .    
```  
  
## 从`huggingface.co`下载模型  
注意, 目前vLLM 支持 macOS apple silicon芯片为实验特性, 并且赞助仅支持FP32 and FP16 datatypes (Currently the CPU implementation for macOS supports FP32 and FP16 datatypes.). 如果模型不是fp32/fp16的, 需要转换, 否则使用时会报错.  
```  
ERROR 02-22 03:00:34 engine.py:140] RuntimeError: "rms_norm_impl" not implemented for 'BFloat16'   
```  
  
https://docs.vllm.ai/en/stable/getting_started/installation/cpu/index.html  
  
1、可以使用cli下载模型, 也可以直接从huggingface.co网站下载模型  
  
https://huggingface.co/docs/huggingface_hub/main/en/guides/cli  
  
https://huggingface.co/models  
  
首先升级python到3.12.x , 可参考 [《mac + mlx-examples 大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章》](../202501/20250108_01.md)     
  
然后安装huggingface cli  
```  
python -m pip install --upgrade pip  
  
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple  
pip config set install.trusted-host mirrors.aliyun.com  
  
pip install -U "huggingface_hub[cli]"  
```  
  
配置宿主机目录, 将模型下载到该目录中  
```  
M_DIR="$HOME/Downloads/model_from_huggingface"  
  
mkdir $M_DIR  
```  
  
使用镜像加速, 下载`Qwen2.5-1.5B`    
```  
M_DIR="$HOME/Downloads/model_from_huggingface"  
MODEL="Qwen/Qwen2.5-1.5B"  
  
HF_ENDPOINT=https://hf-mirror.com nohup huggingface-cli download $MODEL --local-dir $M_DIR/$MODEL --cache-dir $M_DIR/.cache --resume-download --max-workers 4 >/dev/null 2>&1 &  
```  
  
2、可以使用llama.cpp进行转换, 参考:    
- [《手把手教你炼丹 | 用Mac本地微调大模型 , 扩展: 微调数据JSONL的内容格式和什么有关?》](../202502/20250215_01.md)    
  
下载 llama   
```    
cd ~    
git clone --depth 1 git@github.com:ggerganov/llama.cpp.git    
# 或 git clone --depth 1 https://github.com/ggerganov/llama.cpp    
```    
    
编译llama    
```    
cd llama.cpp    
mkdir build    
cd build    
cmake ..    
make -j 8    
```    
    
转换帮助    
```    
cd ~/llama.cpp    
    
$ python3 convert_hf_to_gguf.py -h    
usage: convert_hf_to_gguf.py [-h] [--vocab-only] [--outfile OUTFILE] [--outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}] [--bigendian] [--use-temp-file] [--no-lazy]    
                             [--model-name MODEL_NAME] [--verbose] [--split-max-tensors SPLIT_MAX_TENSORS] [--split-max-size SPLIT_MAX_SIZE] [--dry-run] [--no-tensor-first-split]    
                             [--metadata METADATA] [--print-supported-models]    
                             [model]    
    
Convert a huggingface model to a GGML compatible file    
    
positional arguments:    
  model                 directory containing model file    
    
options:    
  -h, --help            show this help message and exit    
  --vocab-only          extract only the vocab    
  --outfile OUTFILE     path to write to; default: based on input. {ftype} will be replaced by the outtype.    
  --outtype {f32,f16,bf16,q8_0,tq1_0,tq2_0,auto}    
                        output format - use f32 for float32, f16 for float16, bf16 for bfloat16, q8_0 for Q8_0, tq1_0 or tq2_0 for ternary, and auto for the highest-fidelity    
                        16-bit float type depending on the first loaded tensor type    
  --bigendian           model is executed on big endian machine    
  --use-temp-file       use the tempfile library while processing (helpful when running out of memory, process killed)    
  --no-lazy             use more RAM by computing all outputs before writing (use in case lazy evaluation is broken)    
  --model-name MODEL_NAME    
                        name of the model    
  --verbose             increase output verbosity    
  --split-max-tensors SPLIT_MAX_TENSORS    
                        max tensors in each split    
  --split-max-size SPLIT_MAX_SIZE    
                        max size per split N(M|G)    
  --dry-run             only print out a split plan and exit, without writing any new files    
  --no-tensor-first-split    
                        do not add tensors to the first split (disabled by default)    
  --metadata METADATA   Specify the path for an authorship metadata override file    
  --print-supported-models    
                        Print the supported models    
```    
  
转换    
```    
cd ~/llama.cpp   
  
M_DIR="$HOME/Downloads/model_from_huggingface"  
MODEL="Qwen/Qwen2.5-1.5B"  
  
python3 convert_hf_to_gguf.py --outfile $M_DIR/${MODEL}-fp16 --outtype f16 --model-name qwen2.5-1.5b-fp16 $M_DIR/$MODEL   
    
  
  
INFO:hf-to-gguf:Set model quantization version  
INFO:gguf.gguf_writer:Writing the following files:  
INFO:gguf.gguf_writer:/Users/digoal/Downloads/model_from_huggingface/Qwen/Qwen2.5-1.5B-fp16: n_tensors = 338, total_size = 3.1G  
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...  
To disable this warning, you can either:  
    - Avoid using `tokenizers` before the fork if possible  
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)  
Writing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.09G/3.09G [00:10<00:00, 300Mbyte/s]  
INFO:hf-to-gguf:Model successfully exported to /Users/digoal/Downloads/model_from_huggingface/Qwen/Qwen2.5-1.5B-fp16  
```    
  
## 启动vLLM容器, 加载宿主机下载好的模型  
  
1、使用build好的镜像`vllm-apple-arm-env`启动容器, 并加载宿主机下载好的`Qwen/Qwen2.5-1.5B`模型    
  
https://docs.vllm.ai/en/stable/deployment/docker.html  
  
```  
M_DIR="$HOME/Downloads/model_from_huggingface"  
MODEL="Qwen/Qwen2.5-1.5B"  
  
# 注意, --model /data/$MODEL  必须放到命令最后, 是ENTRYPOINT接收的参数, 不是docker run的参数.  
docker run -d -it --name vllm -v $M_DIR:/data -p 8001:8000 vllm-apple-arm-env --model /data/$MODEL   
```  
  
2、查看vLLM容器日志:  
```  
docker logs vllm   
```  
  
3、使用curl查看当前vLLM容器支持的模型  
```  
curl http://localhost:8001/v1/models  
  
{"object":"list","data":[{"id":"/data/Qwen/Qwen2.5-1.5B","object":"model","created":1740193123,"owned_by":"vllm","root":"/data/Qwen/Qwen2.5-1.5B","parent":null,"max_model_len":131072,"permission":[{"id":"modelperm-a1ebaaadb8ab45588c62f23ff944bd9d","object":"model_permission","created":1740193123,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}  
```  
  
chat API例子  
```  
curl http://localhost:8001/v1/chat/completions \  
    -H "Content-Type: application/json" \  
    -d '{  
        "model": "/data/Qwen/Qwen2.5-1.5B",  
        "messages": [  
            {"role": "system", "content": "You are a helpful assistant."},  
            {"role": "user", "content": "你好"}  
        ]  
    }'  
```  
  
容器logs 查看到报错, 原因之前说了  
```  
ERROR 02-22 03:00:34 engine.py:140] RuntimeError: "rms_norm_impl" not implemented for 'BFloat16'  
```  
  
### 使用转换为fp16后的模型    
启动容器  
```  
M_DIR="$HOME/Downloads/model_from_huggingface"  
MODEL="Qwen/Qwen2.5-1.5B-fp16"  
  
# 注意, --model /data/$MODEL  必须放到命令最后, 是ENTRYPOINT接收的参数, 不是docker run的参数.  
docker run -d -it --name vllm -v $M_DIR:/data -p 8001:8000 vllm-apple-arm-env --model /data/$MODEL   
```  
  
```  
curl http://localhost:8001/v1/models  
  
{"object":"list","data":[{"id":"/data/Qwen/Qwen2.5-1.5B-fp16","object":"model","created":1740195680,"owned_by":"vllm","root":"/data/Qwen/Qwen2.5-1.5B-fp16","parent":null,"max_model_len":131072,"permission":[{"id":"modelperm-05c0e9e610844e3db9e96054b5ef62d2","object":"model_permission","created":1740195680,"allow_create_engine":false,"allow_sampling":true,"allow_logprobs":true,"allow_search_indices":false,"allow_view":true,"allow_fine_tuning":false,"organization":"*","group":null,"is_blocking":false}]}]}  
  
  
curl http://localhost:8001/v1/chat/completions \  
    -H "Content-Type: application/json" \  
    -d '{  
        "model": "/data/Qwen/Qwen2.5-1.5B-fp16",  
        "messages": [  
            {"role": "system", "content": "You are a helpful assistant."},  
            {"role": "user", "content": "你好"}  
        ]  
    }'  
```  
  
  
这个卡住了, 不知道为什么, CPU耗费巨大. 跑janus时也遇到类似问题, [《DeepSeek多模态大模型Janus的使用 (文本生成 , 图像生成 , 图像分析), 卡住, 不如DiffusionBee》](../202502/20250208_04.md)    
- [《用tcpdump抓包了解程序到底在访问什么URL, 科学上网/翻墙场景 - 搞定DiffusionBee下载文生图模型》](../202411/20241116_01.md)    
- [《AI生成图片: DiffusionBee/ComfyUI/Diffusers 如何使用stable diffusion在Apple Silicon芯片的Mac上画图（GPU加速）- 文生图text-to-image》](../202411/20241115_02.md)    
  
  
日志如下  
```  
INFO 02-22 03:38:04 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.  
INFO 02-22 03:38:09 metrics.py:455] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.  
```  
  
  
  
更多API例子参考:  
- https://docs.vllm.ai/en/latest/getting_started/quickstart.html  
  

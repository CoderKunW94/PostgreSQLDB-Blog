## 大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章    
                                                                                              
### 作者                                                                  
digoal                                                                  
                                                                         
### 日期                                                                       
2025-01-08                                                        
                                                                      
### 标签                                                                    
PostgreSQL , PolarDB , DuckDB , LLM , MLX , finetuning , 微调 , 大模型 , 蒸馏   
                                                                                             
----                                                                      
                                                                                    
## 背景    
如下文章介绍了如何在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM)? 其用于微调的指令和回复都来自更大参数的模型(例如405b的llama 3), 被微调的模型是更小参数的模型(例如7b的mistral), 有点像用大模型来蒸馏小模型.  因为大模型更加全面, 但是要求的内存和算力都更大, 对于仅用于专业领域的场景来说, 蒸馏的小模型性价比更高.   
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF》](../202407/20240724_01.md)    
  
本文将用我写过的所有关于PolarDB的文章来微调mistral 7b, 看看微调后的模型会不会对PolarDB PG的问题更从容?   
  
我写过的所有关于PolarDB的文章在此: https://github.com/digoal/blog  
  
训练用的机器就是普通的笔记本, 但是要求apple chipset. 可以使用mlx框架( https://github.com/ml-explore/mlx ), 加速训练. mac就是最廉价且触手可及的模型训练和运行机器.   
- Macbook pro m2 16g   
  
## 开始微调  
0、升级python版本  
```
升级到 python 3.12
- https://www.python.org/downloads/macos/   

# 假设已安装到
/Library/Frameworks/Python.framework/Versions/
$ ll /Library/Frameworks/Python.framework/Versions/
lrwxr-xr-x   1 root  wheel     4B Jan  8 17:56 Current -> 3.12
drwxrwxr-x  11 root  admin   352B Jan  8 17:56 3.12

# 添加软链
ln -s /Library/Frameworks/Python.framework/Versions/Current/bin/pip3 /Library/Frameworks/Python.framework/Versions/Current/bin/pip
ln -s /Library/Frameworks/Python.framework/Versions/Current/bin/python3 /Library/Frameworks/Python.framework/Versions/Current/bin/python


# 环境变量
echo "
export PATH=/Library/Frameworks/Python.framework/Versions/Current/bin:\$PATH
" >> ~/.bash_profile


. ~/.bash_profile
```
  
1、把python pip源换成aliyun  
- https://segmentfault.com/a/1190000044200422  
- https://www.jianshu.com/p/71924b5a8aaa  
  
```  
echo "  
[global]  
index-url = https://mirrors.aliyun.com/pypi/simple/  
" > ~/.config/pip/pip.conf  
```  
  
2、准备用于微调的jsonl文件(`train.jsonl`和`valid.jsonl`)  
  
微调不同的模型, 需要不同的jsonl内容格式, 请参考模型说明(通常`ollama` 对应模型 `models`页面中有相关的介绍. 或者进入ollama shell后输入`/show modelfile`), 或者问chatgpt.      
```
$ ollama run mistral:7b
>>> /show modelfile
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this, replace FROM with:
# FROM mistral:7b

FROM /Users/digoal/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435
TEMPLATE """{{- if .Messages }}
{{- range $index, $_ := .Messages }}
{{- if eq .Role "user" }}
{{- if and (eq (len (slice $.Messages $index)) 1) $.Tools }}[AVAILABLE_TOOLS] {{ $.Tools }}[/AVAILABLE_TOOLS]
{{- end }}[INST] {{ if and $.System (eq (len (slice $.Messages $index)) 1) }}{{ $.System }}

{{ end }}{{ .Content }}[/INST]
{{- else if eq .Role "assistant" }}
{{- if .Content }} {{ .Content }}
{{- else if .ToolCalls }}[TOOL_CALLS] [
{{- range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
{{- end }}]
{{- end }}</s>
{{- else if eq .Role "tool" }}[TOOL_RESULTS] {"content": {{ .Content }}} [/TOOL_RESULTS]
{{- end }}
{{- end }}
{{- else }}[INST] {{ if .System }}{{ .System }}

{{ end }}{{ .Prompt }}[/INST]
{{- end }} {{ .Response }}
{{- if .Response }}</s>
{{- end }}"""
PARAMETER stop [INST]
PARAMETER stop [/INST]
LICENSE """
... ...
"""

>>> 
```
  
微调mistral模型为例, jsonl内容格式参考如下:    
- https://github.com/apeatling/simple-guide-to-mlx-finetuning/blob/trunk/train-example.jsonl  
  
jsonl文件内容, 每行的格式:    
```  
{"text":"<s>[INST] 文章标题 [/INST] 文章内容 </s>"}    
```  
  
注意, jsonl使用utf-8编码, 中文会被转义. https://jsonlines.org/    
  
首先下载包含PolarDB文章的blog  
```  
cd ~  
git clone --depth 1 https://github.com/digoal/blog  
```  
  
编辑一个`python`脚本, 脚本内容`~/a.py`附在文末. 从`README.md`中提取PolarDB相关文章的文件名, 从文件名第一行得到文章标题以及文件内容. 并输出为jsonl.     
```  
cd ~  
python3 a.py  
```  
  
结果放到`~/train.jsonl`和`~/valid.jsonl`文件中, 总共176篇, 140篇用来训练, 36篇用来校验.    
```  
$ wc -l output.jsonl   
     176 output.jsonl  
  
$ wc -l train.jsonl   
     140 train.jsonl  
  
$ wc -l valid.jsonl   
      36 valid.jsonl  
```  
  
提取一行jsonl文件的内容看看是否符合前面说的格式要求?   
```  
$ head -n 1 train.jsonl   
{"text": "<s>[INST]\u300a\u6c89\u6d78\u5f0f\u5b66\u4e60PostgreSQL|PolarDB 20: \u5b66\u4e60\u6210\u4e3a\u6570\u636e\u5e93\u5927\u5e08\u7ea7\u522b\u7684\u4f18\u5316\u6280\u80fd\u300b[/INST]## \u6c89\u6d78\u5f0f\u5b66\u4e60PostgreSQL|PolarDB 20: \u5b66\u4e60\u6210\u4e3a\u6570\u636e\u5e93\u5927\u5e08\u7ea7\u522b\u7684\u4f18\u5316\u6280\u80fd   \n  ..... 略去大量文本内容 ..... \n</s>"}  
```  
  
3、克隆mlx-examples项目, 这个项目利用mlx框架加速apple chipset训练. 本例用里面的lora/qlora来微调模型. 详细说明可以参考lora目录中的readme文档.   
```  
cd ~  
git clone --depth 1 https://github.com/ml-explore/mlx-examples.git  
```  
  
安装依赖  
```  
cd ~/mlx-examples/lora  
pip install -r requirements.txt
pip install torch

# 可选:
pip install -U mlx-lm
```  
  
4、微调如下模型   
- https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3  
  
下载模型相关文件前, 需要注册huggingface账号, 同时在模型页面内要授权同意使用该模型, 否则无法下载.    
  
下载  
- https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/tree/main
   
千万不要下载 consolidated.safetensors , 微调时会导致报错. 下载切割的几个safetensors文件即可:     
```
Loading pretrained model
Traceback (most recent call last):
  File "/Users/digoal/mlx-examples/lora/lora.py", line 336, in <module>
    model, tokenizer, _ = lora_utils.load(args.model, tokenizer_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/digoal/mlx-examples/lora/utils.py", line 162, in load
    model.load_weights(list(weights.items()))
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/mlx/nn/layers/base.py", line 178, in load_weights
    raise ValueError(f"Received parameters not in model: {extras}.")
ValueError: Received parameters not in model: layers.25.feed_forward.w1.weight ....
```
  
将相关文件放到目标位置    
```  
mkdir ~/finetune_mistral  
mv ~/train.jsonl ~/finetune_mistral/  
mv ~/valid.jsonl ~/finetune_mistral/  

mv ~/Downloads/config.json ~/finetune_mistral
mv ~/Downloads/model.safetensors.index.json ~/finetune_mistral
mv ~/Downloads/tokenizer.json ~/finetune_mistral
mv ~/Downloads/tokenizer_config.json ~/finetune_mistral
mv ~/Downloads/tokenizer.model ~/finetune_mistral
mv ~/Downloads/params.json ~/finetune_mistral
mv ~/Downloads/special_tokens_map.json ~/finetune_mistral
mv ~/Downloads/generation_config.json ~/finetune_mistral
mv ~/Downloads/model-00001-of-00003.safetensors ~/finetune_mistral
mv ~/Downloads/model-00003-of-00003.safetensors ~/finetune_mistral
mv ~/Downloads/model-00002-of-00003.safetensors ~/finetune_mistral
```  
  
执行微调, 参数影响内存的使用、效果、耗时. 参数设置建议可参考: https://github.com/ml-explore/mlx-examples/tree/main/lora#memory-issues     
```  
cd ~/mlx-examples/lora  
  
python3 lora.py \
  --train \
  --model ~/finetune_mistral \
  --data ~/finetune_mistral \
  --adapter-file ~/finetune_mistral \
  --batch-size 1 \
  --lora-layers 4 \
  --iters 1000  
```
  
一些小问题:     
  
小内存Mac (例如16G) 如果禁用了swap可能会OOM. 解除swap的禁用参考下文:  
- [《禁用 MacOS 的 Swap 分区 - 实测真实有效》](../202212/20221207_01.md)
    ```
    sudo nvram boot-args="vm_compressor=4 serverperfmode=1"  
    ```
      
16G内存可能还是不够, 可以考虑量化后再微调.   
```
Loading pretrained model
Total parameters 7248.130M
Trainable parameters 0.106M
Loading datasets
Training
[WARNING] Some sequences are longer than 2048 tokens. Consider pre-splitting your data to save memory.
上面这个警告和jsonl有关, 可以切分一下内容.
参考: https://milvus.io/docs/zh/how_to_enhance_your_rag.md  

下面这个是 开启swap后还是报错了.
libc++abi: terminating due to uncaught exception of type std::runtime_error: Attempting to allocate 9535252608 bytes which is greater than the maximum allowed buffer size of 8589934592 bytes.
Abort trap: 6
```
   
量化参考:  
- https://zhuanlan.zhihu.com/p/676249749
- https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md
   
如果你的macos版本较低, 可能遇到类似报错   
```  
Traceback (most recent call last):  
  File "/Users/digoal/mlx-examples/lora/lora.py", line 9, in <module>  
    import mlx.core as mx  
ImportError: dlopen(/Users/digoal/Library/Python/3.9/lib/python/site-packages/mlx/core.cpython-39-darwin.so, 0x0002): Symbol not found: _cblas_sgemm$NEWLAPACK  
  Referenced from: <A9854BA1-AE0A-3D86-956A-FA129ACFDAC5> /Users/digoal/Library/Python/3.9/lib/python/site-packages/mlx/lib/libmlx.dylib (built for macOS 13.5 which is newer than running OS)  
  Expected in:     <0494A450-9778-31EB-84F6-88A045586FBF> /System/Library/Frameworks/Accelerate.framework/Versions/A/Accelerate  
  
# 当前为13.2.1 , 更新到13.7.2   
(built for macOS 13.5 which is newer than running OS)  
```  
  
5、微调完成后, 会产出微调后的参数文件`adapters.npz`. 使用该参数文件可以获得微调模型的效果.     
```  
python lora.py --model ~/finetune_mistral \
               --adapter-file ~/finetune_mistral \
               --max-tokens 50 \
               --prompt "这里输入PolarDB相关的问题看看回复效果"   
```  
  
## 附录  
脚本, 准备用于微调的jsonl文件(`train.jsonl`和`valid.jsonl`)  
```  
vi ~/a.py  
```  
  
```  
import re  
import os  
import json  
import random  
  
def get_lines_from_marker(file_path, marker, search_term):  
    with open(file_path, 'r', encoding='utf-8') as file:  
        lines = file.readlines()  
  
    # 查找标记行，并获取从该行开始的所有后续行  
    start_index = None  
    for i, line in enumerate(lines):  
        if marker in line:  
            start_index = i  
            break  
  
    # 如果找到了标记行，筛选从该行开始的所有内容  
    if start_index is not None:  
        filtered_lines = []  
        for line in lines[start_index:]:  
            # 使用正则表达式进行不区分大小写的匹配  
            if re.search(search_term, line, re.IGNORECASE):  
                filtered_lines.append(line.strip())  # strip() 用于去除行末的换行符  
        return filtered_lines  
    else:  
        return []  
  
def process_line(line, output_file):  
    # 使用正则表达式从每行中提取文件路径和标题  
    match = re.match(r'##### (\S+)\s+\[([^\]]+)\]\((\S+)\)', line)  
    if not match:  
        return  # 如果匹配失败，跳过该行  
  
    # 提取文件路径、标题和链接路径  
    file_path = match.group(1)  # 例如 "202412/20241210_01.md"  
    title = match.group(2)  # 例如 "《PolarDB 100 问 | PolarDB 11 使用 pg_bulkload 插件导入数据报错》"  
    link_path = match.group(3)  # 例如 "202412/20241210_01.md"  
      
    # 读取文件内容  
    try:  
        with open("blog/"+link_path, 'r', encoding='utf-8') as file:  
            file_content = file.read()  
    except FileNotFoundError:  
        print(f"文件 {link_path} 未找到，跳过此行。")  
        return  
      
    # 构造 JSON 对象并写入文件  
    json_obj = {  
        'text': f"<s>[INST]{title}[/INST]{file_content}</s>"  
    }  
  
    # 将 JSON 对象写入到输出的 jsonl 文件中  
    with open(output_file, 'a', encoding='utf-8') as out_file:  
        out_file.write(json.dumps(json_obj) + "\n")  
  
def split_jsonl(input_file, train_file, valid_file, train_ratio=0.8):  
    # 读取原始 JSONL 文件内容  
    with open(input_file, 'r', encoding='utf-8') as file:  
        lines = file.readlines()  
      
    # 计算分割点  
    total_lines = len(lines)  
    train_size = int(total_lines * train_ratio)  
      
    # 随机打乱文件内容，确保分割后数据的随机性  
    random.shuffle(lines)  
      
    # 将数据分为训练集和验证集  
    train_lines = lines[:train_size]  
    valid_lines = lines[train_size:]  
      
    # 将训练集写入新的 JSONL 文件  
    with open(train_file, 'w', encoding='utf-8') as train_out:  
        for line in train_lines:  
            train_out.write(line)  
      
    # 将验证集写入新的 JSONL 文件  
    with open(valid_file, 'w', encoding='utf-8') as valid_out:  
        for line in valid_lines:  
            valid_out.write(line)  
      
    print(f"数据分割完成！{train_file} 和 {valid_file} 已生成。")  
  
  
  
file_path = 'blog/README.md'  # 替换为你文件的路径  
marker = "### 六、所有文档如下"  
search_term = "polardb"  # 匹配 'polardb'，不区分大小写  
  
lines_from_marker = get_lines_from_marker(file_path, marker, search_term)  
  
output_file = 'output.jsonl'  # 输出的 JSONL 文件  
  
# 打印包含 'polardb' 的行  
for line in lines_from_marker:  
    process_line(line.strip(), output_file)   
  
print(f"处理完成！输出已保存至 {output_file}.")  
  
train_file = 'train.jsonl'  # 训练集 JSONL 文件  
valid_file = 'valid.jsonl'  # 验证集 JSONL 文件  
  
split_jsonl(output_file, train_file, valid_file)  
```  
  
  
## 参考  
https://github.com/ml-explore/mlx-examples/tree/main/lora#memory-issues   
  
https://github.com/apeatling/simple-guide-to-mlx-finetuning/blob/trunk/train-example.jsonl  
  
https://www.geekyuncle.com/mlx-installation-and-environment-setup/  
  
https://www.geekyuncle.com/part2-mlx-and-yi/  
  
https://milvus.io/docs/zh/how_to_enhance_your_rag.md  
  
切割文本    
  
[《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF》](../202407/20240724_01.md)    
  
https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/tree/main  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  

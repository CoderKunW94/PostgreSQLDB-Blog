## [转载] Part3: 准备数据集  
                                                                                                
### 作者                                                                    
digoal                                                                    
                                                                           
### 日期                                                                         
2025-01-10                                                          
                                                                        
### 标签                                                                      
PostgreSQL , PolarDB , DuckDB , LLM , MLX , finetuning , 微调 , 大模型 , 蒸馏     
                                                                                               
----                                                                        
                                                                                      
## 背景    
原文:   
- https://www.geekyuncle.com/finetuning-local-llm-with-mlx/  
  
为微调大模型准备数据集  
  
# 转载  
  
这篇笔记是以下五个部分中的第3个：[学习笔记：使用 MLX 在 Mac 上微调本地 LLM](../202501/20250110_01.md)。  
  
## 1、数据集的格式  
为了微调 大模型，我们需要训练数据。训练数据将使微调模型产生比单独提示更高质量的结果。 MLX示例项目中的示范训练数据在lora/data目录下，分成了3个数据文件。  
  
1、 `train.jsonl` 训练数据集  
2、 `valid.jsonl` 验证数据集  
3、 `test.jsonl` 测试数据集  
  
训练数据应采用 [JSONL](https://jsonlines.org/) 格式，每一行是一个JSON对象，该对象只有一个'text'元素，其对应的值是按照模型`提示模版`以及`预期的结果`拼接出来的字符串：  
```  
{"text": "<|im_start|>system\n{{ .System }}<|im_end|>\n<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n{{ .Answer }}<|im_end|>"}  
... ...   
{"text": "<|im_start|>system\n{{ .System }}<|im_end|>\n<|im_start|>user\n{{ .Prompt }}<|im_end|>\n<|im_start|>assistant\n{{ .Answer }}<|im_end|>"}  
```  
  
jsonl格式, 准备数据, 也可以参考如下文章. 大的文章可能需要split, 例如 `"{text: system 标题.章节 , assistant 段落}"`    
- [《大模型微调(fine-tuning)实践 - 让它学完所有PolarDB文章》](../202501/20250108_01.md)    
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之5 - 在 Apple Silicon Mac 上微调(fine-tuning)大型语言模型(LLM) 并发布GGUF》](../202407/20240724_01.md)      
  
## 2、数据集的生成  
mlx-examples `lora/data`目录下还有一个`wikisql.py`文件，此文件是用来从`wikisql`项目提供的数据集来生成微调数据集。  
  
我不打算用这个数据集来微调`yi`模型。我们有一个自动生成儿童故事的小程序/网站：[故事酷](https://www.mystory.cool/)。  
  
系统中的故事绝大多数都是用GPT-4来生成的，因此我写了一个小程序，用系统中现有的故事，来生成我们的训练数据集。我希望通过这种方式，能够在这个儿童故事的生成领域，yi模型能够达到chatGPT类似的水平。  
  
生成这个jsonl的方式很多，笔记就不写了，大家可以用自己的方式生成自己的训练集。可以问chatgpt, 写python脚本来实现.    
  
从下几篇训练命令中的`--data`参数看, 作者应该是把几个生成好的 `jsonl` 文件放在了 `mlx-examples/lora/story-data` 目录中.     
  
  
